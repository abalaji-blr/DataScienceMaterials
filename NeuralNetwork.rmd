
## Neural Network

 * Resources
   * **Vidoes**
     + [Data Mining & Artificial Neural Network](https://www.youtube.com/playlist?list=PLea0WJq13cnCS4LLMeUuZmTxqsqlhwUoe)
     + [Google's Free online Deep Learning course at Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPn_OWPFT9ulXLuQrImzHfOV)
     + [Exercise #1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb)
     
     + [Deep Learning by Nando de Frietas](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu)
   
   * **Books**
     + [Online Book : Deep Learning  By Ian Goodfellow, Yoshua Bengio and Aaron Courville](http://www.deeplearningbook.org/)
     + [Online Book: Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
   
   *  **Tutorials**
      + [Tutorial: Deep Learning Tutorial from Stanford](http://ufldl.stanford.edu/tutorial/)
 
 * Blog
 
## Notes

**Types of Neural Network**

1. Feed Forward Network
2. Recurrent Network. (has cycles)
3. Symmetric Network.

## Ariticial Neural Network (ANN)

 [Intro at quora about ANN](https://www.quora.com/What-are-artificial-neural-networks).
 
* Perceptron is an aritifical Neuron. It has inputs and their weights. They are summed up and transformed to get the output.

** Following are different kind of transformations**
  1) Unit step (threshold)
  2) Sigmoid
     + Logistic Sigmoid: Value range from 0 to 1
     + Tangential Sigmoid: Value range from -1 to +1
  3) Piecewise Linear
  4) Gaussian Function (aka bell shaped curve)
  
  

##Convolutional Neural network
 
  This is a type of feed forward neural network, also called as CNN or ConvNet. This models the animal visual perception. This can be applied for visual recognition tasks.
  
  The setup of the network will be something like below:
  
  Input Layer -> Convolution Layer -> Other Hidden Layer -> Output Layer
  
  **The Convolution Layer** : is built using a learnable filter (aka Kernel or patch). The filter is a small neural network (with certain k outputs). The filter scans through the input layer and generates the convolution layer with more depth. This is called convolution. So, network may have multiple convolution layer (basically pyramid structure in terms of increase in the depth, decrease in the image size or spatial information is sequeezed out) before sending it to classifier.
  
  The filter / path, works due to **Statistical Invariance** (Translational invariance). Basically use the bounding box and if the image appears any where in the box, they are treated same. In other words, the share the **same weights**.
  
  [Check the wikipedia for an example](https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Typical_cnn.png)
  
     
 * Building blocks of CNN (aka *ConvNet*)
   * Discrete convolution
   * Pooling
      * Avearge pooling
      * Max Pooling
   * 1x1 Convolution
   * Inception
  
  * Demos / References:
  
    * **[Follow the link LENet (Yann Lecun) for the demos]**(http://yann.lecun.com/exdb/lenet/)
    * **Neural network for image classification (ImageNet)** using CNN by Alex Krizhevsky.
    * [A guide to convolution arithmetic for deep learning by Vincent Dumoulin]()
  
  
## Recurrent Neural Network (RNN)

 * Important liks / blog etc
   + [WildML: Intro to RNN ](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
   + [Blog: Understanding LSTM ](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
   + [Blog: A deep dive into RNN](http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/)
   
   
 * Notes:
   + The idea of RNN is to make use of sequential information. The following are the applications where RNN is used:
     1) Translation from one language to another.
     2) Caption generation from the images.
     3) Speech recognition
     4) Handwriting recognition
   
   + Recurrent neural network has cycles, which means it can "remember" the immediate past event. The next state can be predicted based on the previous state. (Refer to Markov model - markov chain, hidden markov model etc.).
   
   + The backpropagation of the weights (from current time to all previous time, also called as *BackPropagation Thourgh Time [BPTT]*) result in the following problems: *exploding gradient and / or vanishing gradient.* 
   The exploding gradient is resolved by gradient clipping. Basically, limit the norm to certain max value.
   The vanishing gradient is solved to remember only last few prev. states and not all.
   This is achived using LSTM. Note that the LSTM uses gated logic to - write, read & forget (reset).
   
## More RNN

## Multiplicative Neural Network (MRNN)

It is an recurrent neural network. To calculate the weights instead of additive, they are multiplied.

### Echo State Networks (ECN)

The Echo State Networks is a instance of general concept - Reservoir Computing.
The idea behind ECN is to get the benefits of RNN but without facing the problem of RNN - the vanishing gradient & exploding gradient.

The Reservoir computing is the extension to Neural Network. The ECN has a reservoir of sparsely connected neurons with sigmoidal transfer function. The input data is connected to the reservoir. The weights that are trained are the output wegihts which connects from reservoir to output neurons.

## Hopfield Nets

  * Single layer network.
  * It is a Fully connected network except the connection to its own.
  * It is a Recurrent Neural network (RNN)
  * Takes binary inputs; weights are symmetric.
  * It is used to store memories.
  * It is an *energy based model*
  * Used for simple pattern recognition.
  
## Boltzmann Machines

  * It is a Hopfield network with binary stochastic units instead of binary threshold units.
  * It has hidden units as well.
  
  
  