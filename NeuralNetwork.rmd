
## Neural Network

 * Resources
   * **Vidoes**
     + [Data Mining & Artificial Neural Network](https://www.youtube.com/playlist?list=PLea0WJq13cnCS4LLMeUuZmTxqsqlhwUoe)
     + [Google's Free online Deep Learning course at Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPn_OWPFT9ulXLuQrImzHfOV)
     + [Exercise #1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb)
     
     + [Deep Learning by Nando de Frietas](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu)
   
     + [Graphical Visualization of Different Neural Networks](http://www.asimovinstitute.org/neural-network-zoo/)
    
     + [Stanford: Convolutional Neural Network- Karpathy](http://cs231n.github.io/) 
     + [Stanford: Deep learning course on NLP](http://cs224d.stanford.edu/index.html)
     
   * **Books**
     + [Online Book : Deep Learning  By Ian Goodfellow, Yoshua Bengio and Aaron Courville](http://www.deeplearningbook.org/)
     + [Online Book: Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
   
   *  **Tutorials**
      + [Tutorial: Deep Learning Tutorial from Stanford](http://ufldl.stanford.edu/tutorial/)
 
 * Blog / Podcast / Paper
   1. [Deep Learning dot Net](http://deeplearning.net/)
   2. [Talking Machine](http://www.thetalkingmachines.com/blog/)
   3. [Deep Learning Roadmap](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap)
  
## Notes

**Types of Neural Network**

1. Feed Forward Network
2. Recurrent Network. (has cycles)
3. Symmetric Network.

## Ariticial Neural Network (ANN)

 [Intro at quora about ANN](https://www.quora.com/What-are-artificial-neural-networks).
 
* Perceptron is an aritifical Neuron. It has inputs and their weights. They are summed up and transformed to get the output.

** Following are different kind of transformations**
  1) Unit step (threshold)
  2) Sigmoid
     + Logistic Sigmoid: Value range from 0 to 1
     + Tangential Sigmoid: Value range from -1 to +1
  3) Piecewise Linear
  4) Gaussian Function (aka bell shaped curve)
  
  
## Gradient Descent

## Stochastic Gradient Descent

##Convolutional Neural network
 
  This is a type of feed forward neural network, also called as CNN or ConvNet. This models the animal visual perception. This can be applied for visual recognition tasks.
  
  The setup of the network will be something like below:
  
  Input Layer -> Convolution Layer -> Other Hidden Layer -> Output Layer
  
  **The Convolution Layer** : is built using a learnable filter (aka Kernel or patch). The filter is a small neural network (with certain k outputs). The filter scans through the input layer and generates the convolution layer with more depth. This is called convolution. So, network may have multiple convolution layer (basically pyramid structure in terms of increase in the depth, decrease in the image size or spatial information is sequeezed out) before sending it to classifier.
  
  The filter / path, works due to **Statistical Invariance** (Translational invariance). Basically use the bounding box and if the image appears any where in the box, they are treated same. In other words, the share the **same weights**.
  
  [Check the wikipedia for an example](https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Typical_cnn.png)
  
     
 * Building blocks of CNN (aka *ConvNet*)
   * Discrete convolution
   * Pooling
      * Avearge pooling
      * Max Pooling
   * 1x1 Convolution
   * Inception
  
  * Demos / References:
  
    * **[Follow the link LENet (Yann Lecun) for the demos]**(http://yann.lecun.com/exdb/lenet/)
    * **Neural network for image classification (ImageNet)** using CNN by Alex Krizhevsky.
    * [A guide to convolution arithmetic for deep learning by Vincent Dumoulin]()
    * [Stanford CNN course - Karpathy](http://cs231n.github.io/)
  
  
## Recurrent Neural Network (RNN)

 * Important liks / blog etc
   + [WildML: Intro to RNN ](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
   + [Blog: Understanding LSTM ](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
   + [Blog: A deep dive into RNN](http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/)
   
   
 * Notes:
   + The idea of RNN is to make use of sequential information. The following are the applications where RNN is used:
     1) Translation from one language to another.
     2) Caption generation from the images.
     3) Speech recognition
     4) Handwriting recognition
   
   + Recurrent neural network has cycles, which means it can "remember" the immediate past event. The next state can be predicted based on the previous state. (Refer to Markov model - markov chain, hidden markov model etc.).
   
   + The backpropagation of the weights (from current time to all previous time, also called as *BackPropagation Thourgh Time [BPTT]*) result in the following problems: *exploding gradient and / or vanishing gradient.* 
   The exploding gradient is resolved by gradient clipping. Basically, limit the norm to certain max value.
   The vanishing gradient is solved to remember only last few prev. states and not all.
   This is achived using LSTM. Note that the LSTM uses gated logic to - write, read & forget (reset).
   
## More RNN

## Multiplicative Neural Network (MRNN)

It is an recurrent neural network. To calculate the weights instead of additive, they are multiplied.

### Echo State Networks (ECN)

The Echo State Networks is a instance of general concept - Reservoir Computing.
The idea behind ECN is to get the benefits of RNN but without facing the problem of RNN - the vanishing gradient & exploding gradient.

The Reservoir computing is the extension to Neural Network. The ECN has a reservoir of sparsely connected neurons with sigmoidal transfer function. The input data is connected to the reservoir. The weights that are trained are the output wegihts which connects from reservoir to output neurons.

# Graphical Models

## Need for Graphical Models
   Modelling a rich distribution of random variables ( thousands / millions) is a challenging task, computationally and statistically.
   For example, if we take image input of say 16x16 grey scale (binary value of 0 & 1), then the possible parameters are 2^256. In other words, if we have n discrete variables and each have k states, then the required parameters are k^n. This is not feasible because of memory and runtime.
   The table based approach tries to model every possible kind of interaction with every possible subset of variables.
   Usually, most variables influence indirectly. 
   The *Graphical Models* provide a framework to model only direct interactions between the random variables. These Graphical Models are also called as *Structured Probablistic Models*. This requires significantly less parameters.
   
## Directed Models.
   In the Directed Models, the edges are directed. It's a directed graph.
   
   a -> b -> c
   
   The probability distribution of "b" is described as the conditional distribution of "a".
   
   This is also called as *Bayesian network* (uses conditional probability).

## UnDirected Models.
   In the Undirected Models, the edges are undirected. They are also called as *Markov Random Field (MRF)*, *Markov Network*.
   For each *clique* in the undirected graph, a factor called clique potential measures the affinity of the variables in the clique. They provide the unnormalized probability distribution.
   To work with many cliques, it needs to normailised (the final probability should be between 0 to 1). The normalizing constant Z is called *partition funciton*.
   A distribution defined by normailzing a product of clique potentials is called as *Gibbs Distribution*.

### UnDirected Model - Energy Based Model:
   The undirected model depend on assumption p(x) > 0. That is enforced by Energy based model.
   p(x) = exp(-E(x))
   E(x) is Enegry function.
   exp(z) is positive for all z. This gurantees that, for all values of x, the energy function will not be zero.

## Hopfield Nets (HN)

  * Single layer network.
  * It is a Fully connected network except the connection to its own.
  * It is a Recurrent Neural network (RNN)
  * Takes binary inputs; *weights are symmetric.*
  * It is used to store memories.
  * It is an *energy based model*
  * It follows unsupervised learning.
  * Used for simple pattern recognition.
  * It is often called as associate memories.
  
## Boltzmann Machines (BM)
  * It is not a standalone neural network. It is a building block.
  * It is a engery based model.
  * It is a Hopfield network with binary stochastic units instead of binary threshold units.
  * It is also called as *Stochastic Hopfield Net*
  * Binary false is negative 1, (-1) and Binary True is positive 1 (+1).
  * The architecture is similar to Hopfield Net but it has 2 layers - "visible" and hidden.
  * It is a generative model, which means, it can generate the data.
  * The "visible" layer act as input and output (reconstruction).
  * The idea is to capture the input data (from visible units) in higher dimension at the hidden layer. Thus, the input can be reconstructed from the hidden states.
  * The learning process is difficult and time consuming (slow).
  
## Restricted Boltzmann Machines (RBM)

  * It is a special case of Boltzmann Machine.
  * This network also have both "visible" and hidden layers.
  * No connections among the visible neurons.
  * No Connections among the hidden neurons.
  * That is the *restriction* in RBM when compared with BM.
  * In computer lingo, it is a bipartite graph.
  * It is an energy based model. 
  * The cost function involves Partition function in order to normalize the values (ie., get the probabilty between 0 to 1).
  * The computation of Partition function is intractable. So, the joint probability distribution is also intractable.
  * But, computing the conditional probability of hidden units given the visible is simple and we easy to sample from it.
  * We can compute P(hidden|visible) and P(visible|hidden). Then how do we sample from the 
    joint distribution - Gibbs Sampling.
  * [Gibbs Sampling](http://www.umiacs.umd.edu/~resnik/pubs/gibbs.pdf)
  * How do we model the *joint distribution*?
    + Graphical models is one mechanism.
    + By imposing certain constraints, we can answer any query related to those random variables.
  * Contrastive Divergence (CD) : is a recipe for training undirected graphical models. It is an approximate Maximum Likelihood learning algorithm proposed by Geoffery Hinton.
  [More details](http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf)
  
  * [RBM video lecture - Ali Ghodsi](https://www.youtube.com/watch?v=FJ0z3Ubagt4)
  
  * Data Particle
  * Fantasy Particle
  
  * Persistent Contrastive Divergence (PCD)
  * Fast Persistent Contrastive Divergence (FPCD)
  * Gibbs Chain
  
  
## Sigmoid Belief Network (SBN)

  * It is a *generative model*
  * It has both visible and hidden units.
  * The SBN is also *graphical model* like BM. But in the case of SBN, it is a *directed acyclic graph (DAG).*
  * The visible units are at the leaf level.
  * To turn ON one unit, it depends on the state of its parents.
  
## Deep Belief Network (DBN)

 * Uses stack of RBM to form the Deep Belief Network.
 * The output of one RBM becomes the input to next stage RBM.
 * The training process involves, training each RBM one at a time.
 * The trainng process uses Contrastive Divergence, similar to RBM.
 * Some edges are directed and most of edges are undirected.
 
 * [Overview @ scholarpedia](http://www.scholarpedia.org/article/Deep_belief_networks)
 * [More info : Watch youtube video](https://www.youtube.com/watch?v=WKet0_mEBXg)
 
## Autoencoder

 * Is used to learn the representation (encode) of user input data, for the purpose of dimensionality reduction.
 * Widely used to learn *generative models* of data.
 
 * The simplest architecture is - a feedforward, non recurrent neural network.
 * The number of nodes in the input layer and output layer are same.
   Because, output layer is used to reconstruct the input (in other words, decode).
 * These are used for *unsupervised* learning.
 
 * Note that, this is similar to PCA but PCA does only linear transformations.
   Whereas, autoencoder performs *non linear transfromations.*
   
 * The variations of autoencoder are:
     1) Donoising autoencoder
     2) Sparse autoencoder
     3) Variational autoencoder
     4) Contractive autoencoder
     
## Latent Semantic Analysis (LSA)
   LSA is a technique which analyzes the relationships between the set of documents and terms used in them. 
   It uses the *term - document* matrix which describes the terms used in the document.
   The row refers to the term and the column corresponds to document.
   
   The challenges:
    1) Computational complexity: Calculating the SVD (Singular Value Decomposition) matrix. It's time consuming.
    2) Adding new document, which involves recomputing the SVD.
   
     
 