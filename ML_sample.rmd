
# Machine Learning - Sample R code

## Logistic Regression

```{r , eval=FALSE}

# model
library(stats)
model <- glm(Response ~., family = binomial(link = "logit"), data = train)
summary(model)

# prediction
pred <- predict(model, newdata = test, type = "response")

pred <- ifelse(pred > 0.5, 1, 0)

# calculate the MCC (Mathews Corrleation Coefficient)

compute_mcc <- function(actual, pred) {
   # calculate the confusion matrix terms
   TP <- sum(actual == 1 & pred == 1)
   TN <- sum(actual == 0 & pred == 0)
   FN <- sum(actual == 1 & pred == 0)
   FP <- sum(actual == 0 & pred == 1)
   
   print("TP ", TP)
   numer <- (TP*TN) - (FP*FN)
   denom <- sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+TN))
   mcc <- numer/denom
}

mcc_score <- compute_mcc(train$Response, pred)
mcc_score
```

## XGBoost

### Benefits
     * Speed: it can automatically do parallel computation on Windows and Linux, with OpenMP. It is generally over 10 times faster than the classical gbm.
     
    *Input Type: it takes several types of input data:
      +  Dense Matrix: R‘s dense matrix, i.e. matrix ;
      +  Sparse Matrix: R‘s sparse matrix, i.e. Matrix::dgCMatrix ;
      +  Data File: local data files ;
      +  xgb.DMatrix: its own class (recommended).

    * Sparsity: it accepts sparse input for both tree booster and linear booster, and is optimized for sparse input ;
    * [For more info](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html)

### Build Model    
```{r, eval= FALSE}

# separate out the response column
response <- train[, c("Id, "Response"")]

# create xgb sparse matrix input
dtrain <- xgb.DMatrix(data = train[,-1], label = response$Response)

#build the model
# to control overfitting : adjust max_depth, eta (step size), nrounds
# to handle imbalanced dataset: set max_delta_step =1
#
xgbModel <- xgboost(data = dtrain,
                 objective = "binary:logistic",
                 nrounds = 25, # number of passes on data
                 max_depth = 25, # depth of tree
                 verbose = 1, # valid values 0, 1, 2
                 missing = NA, # handle the missing values
                 )

```

### make the prediction    
```{r, eval= FALSE}

pred <- predict(xgbModel, test)

# convert the probability to boolean
prediction <- as.numeric(pred > 0.5)

```

### View feature importance from the model

```{r, eval=FALSE}
importance_matrix <- xgb.importance(model = xgbModel)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
```

### Save / load the XGBoost model

```{r, eval=FALSE}

# save model to binary local file
xgb.save(xgbModel, "xgboost.model")

# load binary model to R
model <- xgb.load("xgboost.model")
pred2 <- predict(model, test$data)
```
