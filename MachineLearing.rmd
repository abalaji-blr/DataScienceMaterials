
## Machine Learning Materials

  * Resources
     + [Top Machine Learning MOOCs](http://www.kdnuggets.com/2016/07/top-machine-learning-moocs-online-lectures.html)
     + [Machine Learning Mastery](http://machinelearningmastery.com/)
     + [Machine Learning Getting Started](http://machinelearningmastery.com/start-here/#getstarted)
     
     * Optimization Algorithms
       + [Intro to Gradient Descent with pictures](https://pmirla.github.io/2016/06/05/gradient-explanation.html)
       + Gradient Descent : Uses all training examples to find out the next descent.
       + Stochastic Gradient Descent: Randomly samples one training example to identify the descent and updates the weights and continues. This is used in the "online" learning as well.
       + When the training examples are more, Stochastic Gradient is used. Otherwise, Gradient descent is used.
       + [Intuition behind Gradient descent](https://www.quora.com/What-is-an-intuitive-explanation-of-gradient-descent)
       + [Intuituion behind Stochastic Gradient Descent](https://www.quora.com/What-is-an-intuitive-explanation-of-stochastic-gradient-descent)
       +[Details: Diff between Gradient Descent & Stochastic Gradient Descent](https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent)
       + [Maths Insight: Derivative of Gradient Introduction](http://mathinsight.org/directional_derivative_gradient_introduction)
     
     * Supervised Learning : SVM
        + [SVM](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM)
        + [SVM with pictures](https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i)
        + [SVM Kernel - Video](https://youtu.be/3liCbRZPrZA)
        + [Video lecture - Bristol University](http://videolectures.net/epsrcws08_campbell_isvm/)
        
        
        
    * Supervised Learning - Boosted Trees:
       +  [Boosted Trees - Slides](http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)
       +  [L2 norm / L1 norm](http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/)
       + [What is a norm in mathematics?](https://www.youtube.com/watch?v=pgJ2Sg1jcYQ)
       + [Xgboost source code](https://github.com/tqchen/xgboost)
       + [XGBoost Tutorial in R ](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html)
       + [XGBoost intro using R](https://www.r-bloggers.com/an-introduction-to-xgboost-r-package/)
       + Xgboost Installation - Get the latest changes from below
```{r, eval=FALSE}
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
```

  * Neural Networks
    + [Types of Neural Network](http://colah.github.io/posts/2015-09-NN-Types-FP/)
    + [Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)
        
  * Time Series 
    + [Video](https://www.youtube.com/watch?v=QHsmAM6nktY&index=5&list=PLCj1LhGni3hOA2q0sfDNKBH9WIlLxXkbn#t=28.418673)
    + [Time Series with R](https://www.researchgate.net/profile/Andrew_Metcalfe6)
    
  * t-SNE
    + [t-SNE plots](http://distill.pub/2016/misread-tsne/)
    
  * Blog
    + [Reddit](https://www.reddit.com/r/MachineLearning)
    + [WildML](http://www.wildml.com/)
    + [Analytics Vidhya Blog](https://www.analyticsvidhya.com/blog)
    
  * References
    + Andrew Ng - Coursera course 
    + [Awesome Machine Learning resources](https://github.com/josephmisiti/awesome-machine-learning)
    
    
## code samples

## sigmoid
```{r eval=false}
# sigmoid definition

sigmoid = function(x) {
  1 / ( 1 + exp(-x))
}

# generate the list
num_list <- seq(-10, 10, 0.1)

# plot them
plot(num_list, sigmoid(num_list), col = 'blue')

```

