
## Machine Learning Materials

  * Resources
     + [Top Machine Learning MOOCs](http://www.kdnuggets.com/2016/07/top-machine-learning-moocs-online-lectures.html)
     + [Machine Learning Mastery](http://machinelearningmastery.com/)
     + [Machine Learning Getting Started](http://machinelearningmastery.com/start-here/#getstarted)
     
     * Optimization Algorithms
       + [Intro to Gradient Descent with pictures](https://pmirla.github.io/2016/06/05/gradient-explanation.html)
       + Gradient Descent : Uses all training examples to find out the next descent.
       + Stochastic Gradient Descent: Randomly samples one training example to identify the descent and updates the weights and continues. This is used in the "online" learning as well.
       + When the training examples are more, Stochastic Gradient is used. Otherwise, Gradient descent is used.
       + [Intuition behind Gradient descent](https://www.quora.com/What-is-an-intuitive-explanation-of-gradient-descent)
       + [Intuituion behind Stochastic Gradient Descent](https://www.quora.com/What-is-an-intuitive-explanation-of-stochastic-gradient-descent)
       +[Details: Diff between Gradient Descent & Stochastic Gradient Descent](https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent)
       + [Maths Insight: Derivative of Gradient Introduction](http://mathinsight.org/directional_derivative_gradient_introduction)
     
     * Supervised Learning : SVM
        + [SVM](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM)
        + [SVM with pictures](https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i)
        + [SVM Kernel - Video](https://youtu.be/3liCbRZPrZA)
        + [Video lecture - Bristol University](http://videolectures.net/epsrcws08_campbell_isvm/)
        
        
        
    * Supervised Learning - Boosted Trees:
       +  [Boosted Trees - Slides](http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)
       +  [L2 norm / L1 norm](http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/)
       + [What is a norm in mathematics?](https://www.youtube.com/watch?v=pgJ2Sg1jcYQ)
       + [Xgboost source code](https://github.com/tqchen/xgboost)
       + [XGBoost Tutorial in R ](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html)
       + [XGBoost intro using R](https://www.r-bloggers.com/an-introduction-to-xgboost-r-package/)
       + Xgboost Installation - Get the latest changes from below
```{r, eval=FALSE}
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
```

  * Neural Networks
    + [Types of Neural Network](http://colah.github.io/posts/2015-09-NN-Types-FP/)
    + [Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)
        
  * Time Series 
    + [Video](https://www.youtube.com/watch?v=QHsmAM6nktY&index=5&list=PLCj1LhGni3hOA2q0sfDNKBH9WIlLxXkbn#t=28.418673)
    + [Time Series with R](https://www.researchgate.net/profile/Andrew_Metcalfe6)
    
  * t-SNE
    + [t-SNE plots](http://distill.pub/2016/misread-tsne/)
    
  * Blog
    + [Reddit](https://www.reddit.com/r/MachineLearning)
    + [WildML](http://www.wildml.com/)
    + [Analytics Vidhya Blog](https://www.analyticsvidhya.com/blog)
    
  * References
    + Andrew Ng - Coursera course 
    + [Awesome Machine Learning resources](https://github.com/josephmisiti/awesome-machine-learning)
    
    
## code samples

## sigmoid
```{r eval=false}
# sigmoid definition

sigmoid = function(x) {
  1 / ( 1 + exp(-x))
}

# generate the list
num_list <- seq(-10, 10, 0.1)

# plot them
plot(num_list, sigmoid(num_list), col = 'blue')

```

## Machine Learning Notes

### Supervised Learning

From the labeled dataset, gleaning info. from it. In the mathematical terms, it's called *function approximation* to generalize. In other words, from the traning examples, identify the pattern and formulate the expression. It's also called *induction* ( ie., specific set of examples to *generalization*)

#### Classification
 Mapping the input to discrete set of labels (True/ False, Male/ Female etc).
 
##### Decision Trees

  * Representation
  
  It is a tree. The nodes represent the *attributes* or *indepdent variables* from the dataset. The edges represent the *value*.
  
  
  * Algorithm (ID3 / C4.5)
  
  Word Dichotomise : separate into two parts.
  ID3 : Iterative Dichotomiser 3 (probably version 3?). 
  C4.5 is extension to ID3, which can be used for *classification*. Also, called as *Statistical Classifier*.
  
  Entropy: measure of randomness / uncertainity. Entropy is zero when the outcome is certain.
  
  For a given dataset S, the entropy is defined as H(S) = - p(x) log(x). See wikipedia more info.
  
  When entropy is zero, ie., H(S) = 0, the dataset S is perfectly classified.
  
  
  Information Gain: is a measure which describes how much the uncertainity is reduced due to splitting by a certain attribute.
  
  So, algorithm has to maximize the information gain.
  
 
#### Regression
 Mapping the input to value ( some real number).

### Unsupervised Learning

Provided with bunch of data, you need to find the *structures* among them. In otherwords, we need to group them based on certain things.

### Reinforment Learning

