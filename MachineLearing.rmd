
## Machine Learning Materials

  * Resources
     + [Top Machine Learning MOOCs](http://www.kdnuggets.com/2016/07/top-machine-learning-moocs-online-lectures.html)
     + [Machine Learning Mastery](http://machinelearningmastery.com/)
     + [Machine Learning Getting Started](http://machinelearningmastery.com/start-here/#getstarted)
     + [CalTech: Machine Learning Video Library](https://work.caltech.edu/library/)
     
     * Optimization Algorithms
       + [Intro to Gradient Descent with pictures](https://pmirla.github.io/2016/06/05/gradient-explanation.html)
       + Gradient Descent : Uses all training examples to find out the next descent.
       + Stochastic Gradient Descent: Randomly samples one training example to identify the descent and updates the weights and continues. This is used in the "online" learning as well.
       + When the training examples are more, Stochastic Gradient is used. Otherwise, Gradient descent is used.
       + [Intuition behind Gradient descent](https://www.quora.com/What-is-an-intuitive-explanation-of-gradient-descent)
       + [Intuituion behind Stochastic Gradient Descent](https://www.quora.com/What-is-an-intuitive-explanation-of-stochastic-gradient-descent)
       +[Details: Diff between Gradient Descent & Stochastic Gradient Descent](https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent)
       + [Maths Insight: Derivative of Gradient Introduction](http://mathinsight.org/directional_derivative_gradient_introduction)
     
     * Supervised Learning : SVM
        + [SVM](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM)
        + [SVM with pictures](https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i)
        + [SVM Kernel - Video](https://youtu.be/3liCbRZPrZA)
        + [Video lecture - Bristol University](http://videolectures.net/epsrcws08_campbell_isvm/)
        
        
        
    * Supervised Learning - Boosted Trees:
       +  [Boosted Trees - Slides](http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)
       +  [L2 norm / L1 norm](http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/)
       + [What is a norm in mathematics?](https://www.youtube.com/watch?v=pgJ2Sg1jcYQ)
       + [Xgboost source code](https://github.com/tqchen/xgboost)
       + [XGBoost Tutorial in R ](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html)
       + [XGBoost intro using R](https://www.r-bloggers.com/an-introduction-to-xgboost-r-package/)
       + Xgboost Installation - Get the latest changes from below
```{r, eval=FALSE}
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
```

  * Neural Networks
    + [Types of Neural Network](http://colah.github.io/posts/2015-09-NN-Types-FP/)
    + [Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)
        
  * Time Series 
    + [Video](https://www.youtube.com/watch?v=QHsmAM6nktY&index=5&list=PLCj1LhGni3hOA2q0sfDNKBH9WIlLxXkbn#t=28.418673)
    + [Time Series with R](https://www.researchgate.net/profile/Andrew_Metcalfe6)
    
  * t-SNE
    + [t-SNE plots](http://distill.pub/2016/misread-tsne/)
    
  * Blog
    + [Reddit](https://www.reddit.com/r/MachineLearning)
    + [WildML](http://www.wildml.com/)
    + [Analytics Vidhya Blog](https://www.analyticsvidhya.com/blog)
    
  * References
    + Andrew Ng - Coursera course 
    + [Awesome Machine Learning resources](https://github.com/josephmisiti/awesome-machine-learning)
    
    
## code samples

## sigmoid
```{r eval=false}
# sigmoid definition

sigmoid = function(x) {
  1 / ( 1 + exp(-x))
}

# generate the list
num_list <- seq(-10, 10, 0.1)

# plot them
plot(num_list, sigmoid(num_list), col = 'blue')

```

## Machine Learning Notes

### Supervised Learning

From the labeled dataset, gleaning info. from it. In the mathematical terms, it's called *function approximation* to generalize. In other words, from the traning examples, identify the pattern and formulate the expression. It's also called *induction* ( ie., specific set of examples to *generalization*)

#### Classification
 Mapping the input to discrete set of labels (True/ False, Male/ Female etc).

##### Terminologies
    1) *Bias Error* are the simplfying assumptions made by a model by the target function easier to learn.
    
    *Low Bias*: Suggests more assumptions about the form of the target function.
    
    *High Bias*: Suggests less assumptions about the form of the target function.
    
    2) *Variance Error* is the amount that the estimate of the target function will change if the different training data is used.
    
    *Low Variance* : Suggests small changes to the estimate of the target function with changes to the training dataset.
    
    *High Variance* suggests large changes to the estimate of the target function with changes to the training dataset.
    
    3) *Bagging* : also called as Boostrap AGGregatING (BAGGING). *Boostrap* means, a random subset of sampling (with replacement) of training data is used for the learning process. These are repeated multiple times. The results from different learners are averaged (mean) to find out the prediction. In the case of classification problem, the majority of the votes, decides the result class.
    
    *Sampling with replacement* means, during the subset sampling, the training examples will left in the training dataset. So, the duplicate examples can be there in the subset as it is a random process.
    
    Bagging Algorithm: *Random Forest*
    
    4) *Boosting* is a procedure which combines the group of *weak* learners. The algorithm sequentially applies the *weak learning* algorithm repeatedly to the *modified* versions of the training data. The prediction from all will be combined through a *weighted* majority vote for the final prediction.
    
    Refer Book : The Elements of Statistical Learning 
    
    5) Correlation
    6) Co-variance

##### Decision Trees

  * Representation
  
  It is a tree. The nodes represent the *attributes* or *indepdent variables* from the dataset. The edges represent the *value*.
  
  
  * Algorithm (ID3 / C4.5)
  
  Word Dichotomise : separate into two parts.
  ID3 : Iterative Dichotomiser 3 (probably version 3?). 
  C4.5 is extension to ID3, which can be used for *classification*. Also, called as *Statistical Classifier*.
  
  Entropy: measure of randomness / uncertainity. Entropy is zero when the outcome is certain.
  
  For a given dataset S, the entropy is defined as H(S) = - p(x) log(x). See wikipedia more info.
  
  When entropy is zero, ie., H(S) = 0, the dataset S is perfectly classified.
  
  
  Information Gain: is a measure which describes how much the uncertainity is reduced due to splitting by a certain attribute.
  
  So, algorithm has to maximize the information gain.
  
  The Decision Tree Learning is as follows:
  
  1. Pick Best Attribute. Best -> splits the data well
  2. Answer Asked question.
  3. Follow the answer path.
  4. Repeat step 1, until you got an answer.
  
##### Random Forest
  Random Forest builds many de-correlated trees and averages the result based on them. This is also called as Bagging or Boostrap Aggregation.
  
  The idea is to average the noisy but unbiased models, which reduces the variance as well.
  
  When the Random Forest is used for classification problem, the *votes* are obtained from each tree and then classifies using the majority vote. When used for regression, the prediction from each tree is averaged to get the final prediction.
  
  The *variable importance* can be obtained through this algorithm. The variable importance is calculated using *GINI* coefficient. It is a measure of "inequality" in the distribution. It's value range from 0 to 1. When Gini coefficient is zero means, the inequality is zero, ie., distribution is equal. When it's one, the inequality is one, that is distribution is not equal. Refer to *Lorenz Curve* as well as they are related.
  
  * Links:
    + [Leo Breiman: Random Forest Overview](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#overview)
    + [Adele Cutler: Random Forest Overview](http://www.math.usu.edu/~adele/RandomForests/index.htm)
  
  

#### Regression
 Mapping the input to value ( some real number).

### Unsupervised Learning

Provided with bunch of data, you need to find the *structures* among them. In otherwords, we need to group them based on certain things.

### Reinforment Learning

