
## Pre-requisites

  *  Maths
      + [Reltionship between Logarithms & Exponents](http://www.purplemath.com/modules/logs.htm)
      
  * Calculus - is about **Limits**. Limits allow us to see how things **change**. A branch of calculus deals with the "curves & slopes" is called **Differential Calculs*. The slopes are nothing but tangent lines at a point. These slopes are called **derivaties** or **gradient**. 
  The other branch of Calculus deals with "areas (or volumes)" is called "integrals". For example, find out the area under the curve? etc.
  
     + [Calculus- What is it?](https://www.youtube.com/watch?v=INFoNeC1eBA)
     + [Differentiation / Derivaties / Gradient ](https://www.youtube.com/watch?v=Dq3vqarm-VQ)
      
      + Geometry : Deals with Shapes ( circumference, area etc)
      
      + Trignometry : Deals with triangles (interior angles, hypotenuse)
      
      + Linear Algebra : Deals with equations
      
      + Matrices
      
      + Numerical Computation
   
   * Information Theory
   
## Linear Algebra 
  
  The following are the different mathematical objects: Scalar, Vector, Matrices and Tensor (array with more than two axes).
  
  Matrix operations: The matrix operations provides a *transformation* (scale / rotate / shear / flip) of the data.
  
   0) Addition C = A + B
   1) Subtraction C = A - B
   2) Product C = AB
   2b) Determinant of a matrix
       * It is just a number
       * Determinant can be calculated only for square matrices.
       * If the determinant is zero, then there won't exist the inverse matrix and
         that matrix is called *singular* matrix.
         
       * Invertible Matrix: Each point in one space can be transformed to a point in the newly transformed space. The transform of these points can be reversed. Such transformation matrix is called *Invertible Matrix*.
       
       * Non-invertible matrix: When many points in one vector space transforms to the same point in the destination space. Then we can't reverse those transformations.
       
       * det(AB) = det(A).det(B)
       * [what is a determinant mean](https://www.quora.com/What-does-the-determinant-of-a-matrix-mean)
       * [What is a determinant : explanation with pictures](https://arcsecond.wordpress.com/2012/01/24/what-is-a-determinant/)
       
       
   3) Distributive A(B+C) = AB + AC
   4) Associative A(BC) = (AB)C
   5) Identity Matrix AI = A,
      Where I is identify matrix, the main diagonal elements are ONE.
   6) Inversion (A^-1)A = I (or) A(A^-1) = I
   7) Norm: Length of the vector.
       L1 norm:
       L2 norm:
   8) Special Matrices & Vectors:
      a) Diagonal matrix
      b) Orthogonal matrix : (A^T)A = A(A^T) = I
         which implies that (A^T) = (A^-1)
         
    9) EigenDecomposition
       * Eigen Vector: The word *Eigen* is a German word, meaning *self / own / specific / inherient*. The vector is does not change the direction of the transformation is called eigen vector.
       A matrix can be decomposed into eigen vector and eigen values.
       
    10) Singular Value Decomposition (SVD)
        The SVD provides way to decompose matrix into singular vector and singular value.
        
        A = UD(V^T)
        
        where A => mxn matrix
              U => mxm matrix, orthogonal matrix
              D => mxn matrix, diagonal matrix
              V => nxn matrix, orthogonal matrix
              
        The values along the diagonal D are called *singular values*.
        The columns of U are called *left-singular vectors*.
        The columns of V are called *right-singular vectors*.
      
## Numerical Computation
   This deals with solving problems in a iterative process instead of analytically deriving a formula.
   
   1) Underflow: occurs when numbers are of near zero are getting rounded. Note that division by zero results in NaN (not a number)
   2) Overflow : occurs when numbers are of larger magnitude. The arithmetic operation may result into Nan.
   3) Softmax : this avoids both underflow and overflow problems.
   4) Gradient based optimization:
       The steepest descent converges when the gradient is zero (close to zero).
   5) Jacobian Matrix: Contains the partial derivatives of both input and output.
   6) Hessian Matrix: Contains the second order derivative.

## Probablity & Statistics
     + [Statistics explained with pictures](http://www.mathsisfun.com/data/index.html)
     + [Online lectures](http://www.ilectureonline.com/lectures/subject/MATH/18/164/1670)
     + Basics
       + Conditional Probability (Conditioning)
         Two dependent events.
         P(E|F) => Probability of E given F.
         P(lung cancer | smoker) - the probability of having lung cancer given that you are a smoker.
       + *Bayes's Theorem*
         P(A|B) = P(A & B) / P(B) 
                = P(B|A)P(A) / (P(B|A)P(A) + P(B|~A)P(~A))
          
         The conditional prob P(B|A) is also called as *likelihood*
         The *log-likelihood* is nothing but log(P(B|A)).
         
         In general, the log likelihood is preferred in Machine learning because, it is easy to calculate the derivatives.
         
       + Marginalisation

       + Co-variance
       
    * Probability Distribution
      + The Normal Distribution
        + Also known as Gaussian Distribution
      + The Binomial Distribution
      + Central Limit Theorem
    * Hypothesis Testing
      + Confidence interval
      + p-value => probability value
    * The Correlation coefficient
       
  
## Descriptive Statistics

Descriptive statistics  are used to describe the basic features of the data in study.
They provide the quantitative data in a manageable form. ie., simplify the large amount of data in a sensible way.

There are three characterstics for a single variable that we tend to look at:

   1) *Measures of Central Tendency*
         + Also, known as measures of location.
         + The idea of central tendency is to give you the idea of what a typical value for a variable.
         + Mean => average value
         + Median  => sort the values in ascending / decending order and pick the mid value
         + Mode => frequently occuring value
    
    2) *Measures of Dispersion*
         + It refers to how much the value of the variable is "spread out".
         + Following are some measures of Dispersion:
           + Range => difference between the highest and lowest value
           + Interquartile Range => difference between 75th and 25th percentile values.
           + Variance, Standard Deviation:
              It shows how much the individual values in a dataset are vary from mean / average value.
 
     3) *Distribution*
         It is the summary of the frequency of individual or range of values for a variable.
The different kinds of distributions are:

       1) Uniform Distribution
       2) Berouli Distribution (aka Binomial)
       3) Multinomial Distribution
       4) Gaussian Distribution (aka Bell Curve)
       
### Correlation
  Correlation describes the relationship between the two *variables*. The relationship refers to the *corresspondence* between the two variables. The correlation relationship simply says that the two variables perform in a synchronised manner.
  
  
## Inferential Statistics
   Inferential statistics helps to make inference from data. The major inferential statistics come from statistical model called *General Linerar Model (GLM)*.
   
   The GLM provides the foundation for the following statistical methods:
   
   1) t-test
   2) Analysis of Variance (ANOVA)
   3) Analysis of CoVariance (ANCOVA)
   4) Regression Analysis
   5) MultiVariate Methods:
       a) Factor Analysis
       b) Cluster Analysis
       c) Multidimensional Scaling
       d) Discriminant Function Analysis
       e) Canonical Correlation

### Probability Distribution
    Is a description of how likely a random variable or set of random variables is to take on each of its possible states. The values can be discrete or continuous.
    
#### Discrete variables and Probability Mass Function
    The probability distribution over discrete varialbes are described using *Probability Mass Function*. 
    Ex: 
    1) P(x = x) 
    2) P( x= x, y = y)   : This is probability joint distribution as multiple random variables are involved.
    
#### Continuous variables and Probability Density Function
   The probability density function does not give the probability of a specific state directlly, instead it provides the probability of landing inside the region.
   
#### Marginal Probability
   Let's say, we know the probaility distribution over a set of variables. But, when we want to know the probability distribution for a subset of variables, it is called as *Marginal Probability*.
   
#### Conditional Probability
     This describes the probability of certain event given that some other event has happened. They are used to describe the dependent events.
     
#### Chain Rule (or Product Rule) of conditional probabilities.
   Any join distribution over many random variables can be decomposed into conditional distribution over only one variable.
   
   Ex: P(a, b, c) = P(a | b,c). P(b,c)
       P(b,c) = P(b | c) . P(c)
       so, P(a,b,c) = P(a|b,c).P(b|c).P(c)

#### Expectation, Variance and Covariance
   The Expectation or expected value of some function f(x) with respect to a probability distribution P(x) is the average or mean value.
   The Variance gives a measure of how much the value of the function of random variable vary as we sample different values from its probability distribution.
   The *Covariance* gives some sense of how much the two values of two random variables are linearly related to each other.
   
#### Common Probabilty Distributions
   1) Bernoulli Distribution : (aka) Binomial Distribution. (two states, 0 / 1, True / False)
   2) Multinoulli Distribution : (aka) Multinomial Distribution ( k states)
   3) Gaussian Distribution : Bell Curve, Normal Distribution
   4) Exponential Distribution: To model sharp peak at x = 0.
   
#### Common Functions
   1) Logistic Sigmoid = 1 / (1+ e(-x)) [ Models, Binomial distribution]
   2) Softplus function = log( 1 + exp(x)) [ useful to mean & variance in Normal Distribution]
   
### Probability & Statistics FAQs:

  * What is difference between Probablity & Statistics?
  
     Probability: is a mathematical framework to represent uncertainty.
     Statistics:
     
  * What is difference between *discrete and continuous* probability distribution?
  
    Discrete Distribution :  the variable can take a value from a set of values.
    Continuous Distribution: Any value in the range.

 * What is the difference between *prior and posterior* in statistics?
   [ Refer to Bayesian statistics]
   *Prior*: is the probability distribution that represents the uncertainity over your interested variable before you have sampled any data and tried to predict it.
   
   *Posterior*: is the probability distribution that represents the uncertainity over the interested variables after sampling. It is a conditional distribution because it conditions of the sampled data.
   

## Information Theory
  Information theory is a branch of applied mathematics which revolves around the quantifying how much information is present in the signal.
  
  The basic intution behind Information theory for Machine Learning is that learning an unlikely event has occurred is more informative than the likely event has happened.
  
  The information is quantified in such a way that
    a) Likely events will have less information content.
    b) Less Likely events will have more information content.
    c) Independent events should have additive information.
    
    The self-information is defined as I(x) = -log P(x), 
    where I(x), self-information measured in *nats*.
          P(x) is the probability of event x.
          
### KL Divergence
   If we have two different probability distributions P(x) and Q(x) for the same random variable, we can measure how different these two distributions are using Kullback Liebler Divergence.
   
### Cross Entropy
   Cross Entropy is used as an error measure instead of squared error.
## Computer Science
  
     + [Interactive SQL Tutorial](http://sqlzoo.net/wiki/SQL_Tutorial)
     + [SQL Tutorial](https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/)
     + [SQL Joins](https://www.youtube.com/watch?v=KTvYHEntvn8)
        
     
